predict_objs (time t)
    for each object
        object.state(t) = object.predict_model(t)
    

class object_point_type
{
    location_type location;
    color_type color;
}

class object_move_type
{
    /*
        location change,
        send control signal to its joints
        communicating message
    */
    
    object_start_state;
    object_end_state;
        
    vector3 acceleration;
}

class relation_type
{
    location_type location;
    vector3 orientation;
    object_move_type calculate_object_move(relation_type)
    {
    }
}

class object_type
{
    list<object_point_type> points;
    list<relation_type> relations;
    
    // todo: object may contain other objects (subobjects), which move together with the parent object.
    
    predict_model(time t)
    {
       // todo: this prediction model is going to be used somewhere, but where?
        acceleration(t) = force/mass;
        velocity(t) = object.velocity(t_prev)+object.acceleration(t)*(t-t_prev);
        location(t) = location(t_prev) + velocity(t)*(t-t_prev);
    }
    
    add_relation(relation_type relation )
    {
        relations.push_back(relation);
    }
    
    add_point(object_point_type point)
    {
        points.add_point(point);
    }
    
    bool valid()
    {
        return (points.size()>threshold);
    }

    relation_type find_matching_relation(relation_type relation)
    {
    }
    
    update_state(object_move_type object_move)
    {
    }
    
    event_type calculate_event_from_relation_move(observed_scene_type observed_scene, relation_type relation)
    {
        relation_type matching_relation = find_matching_relation(relation);
        object_move_type object_move = matching_relation.calculate_object_move(relation);

        for (auto object_point in points) 
        {
            if (!observed_scene.contains_point(object_point.move(object_move))
                return null;
        }

        object.update_state(object_move);
        
        event_type event(object, object_move, observed_scene.time);
        return event;
    }    
}

class control_signal_type
{
    joint_type joint;
    signal_value_type value;
}

class event_type
{
    control_signal_type control_signal;
    object_type object;
    object_move_type move;
    time_type time;
    
    bool control_signal_event = false;
    
    event_type(object_type object, object_move_type move, time_type time)
    {
        this->object = object;
        this->move = move;
        this->time = time;
    }
    
    event_type(control_signal_type control_signal, time_type time)
    {
        this->control_signal = control_signal;
        this->time = time;
        control_signal_event = true;
    }
    
    bool occurred()
    {
        if (object.state == move.object_end_state)
            return true;
            
        return false;
    }
    
    execute_and_monitor()
    {
        if (control_signal_event)
        {
            send(control_signal.joint, control_signal.value);
            return;
        }
    }
}

class plan_type
{
    list<plan_type> plan_steps;
    
    event_type cause;
    event_type effect;
    //todo: need to add precondition? plan/prediction_model: object precondition and cause, object_move, object's end state (effect)
    // todo: messaging (voice, gesture, ...) is a type of a cause of an intelligent object move
    /* todo: an expected effect may not always follow a cause. In such cases, an additional hidden cause can be presumed. Such hidden cause may be an effect of another observable or hidden cause.
             A plan to achieve an effect may be a graph with multiple causes and side effects:
             cause - node - node - node - node - node - node - effect
                    cause ---|                           |--- side_effect
                             |-- side_effect
       Some nodes (causes and effects) of the graph can be hidden. In such cases, they need to be estimated. For example, a human may be corrupted, evil, hungry, etc.
       The hidden events (or states) in intelligent systems may be made explicit using verbal messages.
       
        events:
        visual observations measure points locations and color. Corresponding events are changes in point locations and color: 
            (point_location(curr_time)=>point_location(future_time)), (point_color(curr_time)=>point_color(future_time))
            This assumes that a point is identified and its location is traced in time.
            The observation describes a point by its properties (location, color)(time) and by the properties (location, color)(time) of surrounding points.
            A decision needs to be made: given the  location and color of a point and its neighbourhood at tau < t and unidentified point and its neighbourhood at time t, 
            find a point in the past (tau < t), which has the same identity.
            It's possible that some set of points at curr_time can be mapped to a set of points in the future_time even though not all individual points of the set can be precisely mapped:
            some points are mapped exactly, some approximately.
            Decision-making is a process resulting in the selection of a belief or a course of action among several alternative possibilities (https://en.wikipedia.org/wiki/Decision-making)
            Selection of the map's target point is made among the points of the scene observed at the future_time.
            Finding the mapping function can be looked upon as an optimization problem: find f(x):X->Y, such that f = min (by g) J(g), here f is a function, which minimizes the criteria J by g, 
            where g is from some class of functions. The calculation performed by the optimal function f could be multi-step. For example, given the f's domain D and range R, and a point x from D, 
            step 1: find points in R, which are close by location to x; step 2: find a point in R among those selected at step 1, which is closest by color to the point x. Instead of finding 
            an optimal choice, the decision can be to find just a good choice. 
            
            The identification of a point (in the current scene with a point in the previous scene) and finding causes of its location and color change and how they change may be solved 
            simultaneously. Or it could be solved in a loop:
            
            observe the scene;  
            observe the scene;              
            find identification model for point identities;
            identify points in the scene, matching them to the points in the previous scene, thus creating point identities, containing point time traces
            find the prediction models of the point identities in the future scenes: prediction(cause)=effect;
            loop
            {
                observe the scene;  
                predict points in the scene from the point identities;  
                identify points in the scene, matching points to the predictions from point identities;
                identify remaining unidentified points in the scene, matching them to the points in the previous scenes, thus creating point identities, containing point time traces
                if error in point prediction   
                {
                    update identification models and the prediction models of the point identities
                }
            }
            
            Example: Identification and prediction in the scene with one point (location, color).
            //observe the scene;  
            time=0; 
            capturing scene(time)={(time,location(time), color(time))}

            //observe the scene;              
            time=time+1; 
            capturing scene(time)={(time,location(time), color(time))}

            //find point identification models for point identities;
            identification_with_previous_scene_points = 
            { [](time} 
              { 
                for each point in scene[time]
                {
                    if (norm(point.location- scene(time-1).point.location) < tolerance_location && norm(point.color- scene(time-1).point.color) < tolerance_color)
                    {
                        point_identity.add_points(scene(time-1).point, scene(time).point);
                        point_identities.add(point_identity);
                        return point_identity;                            
                    }
                }
              }
            }
        
            identification_with_point_identities = 
            { [](time} 
              { 
                for each point in scene[time]
                {
                    for each point_identity in point_identities
                    {
                        predicted_point = point_identity.prediction(time);
                        point_identity.prediction_error.location = norm(point.location- predicted_point.location);
                        point_identity.prediction_error.color = norm(point.color- predicted_point.color);
                        if (point_identity.prediction_error.location < tolerance_location && point_identity.prediction_error.color < tolerance_color)
                        {
                            point_identity.add_point(scene(time).point);
                            return point_identity, prediction_error.location, prediction_error.color;
                        }
                    }
                }
              }
            }

            //identify points in the scene, matching them to the points in the previous scene, thus creating point identities, containing point time traces
            call identification_with_previous_scene_points(time);

            //find the prediction models of the point identities in the future scenes: prediction(cause)=effect;
            /*
                prediction model calculates values of point_identity properties. 
                So finding the prediction model means finding the function or procedure, 
                which performs the calculation of location(time) and color(time). 
                The process of finidng the prediction model for a point_identity maps point_identity to a procedure (program), 
                which input is the point_identities data and output the point_identity data at a future time.
            */
            for each point_identity in point_identities
            {
                point_identity.prediction = 
                { [](time} 
                  { 
                    location = location(time-1) + 1*(location(time-1)-location(time-2);
                    color = color(time-1)
                    return location, color
                  }
                }
            }

            loop
            {
                //observe the scene;  
                time=time+1; 
                capturing scene(time)={{time, location(time), color(time))}
                
                //predict points in the scene from the point identities;  
                //identify points in the scene, matching points to the predictions from point identities;
                call identification_with_point_identities(time);

                //identify remaining unidentified points in the scene, matching them to the points in the previous scenes, thus creating point identities, containing point time traces
                call identification_with_previous_scene_points(time);                

                //if error in point prediction   
                for each point_identity in point_identities
                {
                    if (point_identity.prediction_error.location > tolerance_location || point_identity.prediction_error.color > tolerance_color)
                    {
                        //update identification models and the prediction models of the point identities
                        // identification_with_previous_scene_points and identification_with_point_identities are not improved here, because the example assumes only 1 point in the world
                        point_identity.prediction = 
                        { [](time} 
                          { 
                            // the new calculation includes hidden variables, polynomial time dependency, products and sums of point_identities data
                            // to make the location and color better fit the data
                            // the acceleration may change independently of the point_identities data under action of external forces
                            // if the change of the acceleration is tied to external forces, then an external planner would be able to modify 
                            // the location trajectory of the point according to a goal trajectory
                            acceleration = a_constant; 
                            delta_t = 1;
                            velocity = velocity(t-1) + delta_t*acceleration;
                            location = location(time-1) + delta_t*velocity(t-1) + acceleration(t-1)* delta_t^2/2;
                            color = color(time-1)
                            return location, color
                          }
                        }
                    }
                }
            }
            A point identification may start with its comparison with the predicted identified points. The miscompare may occur because because the point 
            has never been identified in the past or the prediction of a previously identified point failed.
            An identified point combines sighting of the point in past scenes {(location,color)(t), for some past t}
            If the identification and prediction laws in the loop are selected by uniform random sampling, then the chances of finding the right ones are very slim.
            The recurrent identification and prediction should be a converging process, not necessarily strictly monotonicallly.
            Intuitively, the identification of a point in the curr_time scene, depends on:
            - the time continuity of the point location and color:
            -- which entails the proximity in the location and color to the point of the same identity in the prev_time scene;
            -- the similarity of color distribution in the point nbhd in the curr_time and prev_time scenes.
            - the similarity in distance in location space to other identified points;
           
            The prediction of a point location comes from Newton laws, i.e. position and velocity of the point at prev_time and acceleration from the forces acting 
            on the point, including from nbhd points, e.g. X(t)=X0+V0*t+a0*t^2/2;
                        
            A cost can be associated with an option. When the option's cost is lowest or/and below a threshold, the option is selected.
            Selection of a point at future_time, which is close by color and location from a point at curr_time is motivated by a belief (hypothesis) 
            that a material point is not likely to change its properties very fast. This hypothesis is confirmed later by ability to predict a point behaviour in most of the cases.
            
        tactile observations measure points location and temperature. Corresponding events are changes in point locations and temperature.
        auditory observations measure points location, sound volume and frequency. Corresponding events are changes in point locations, volume and frequency. 
        
        Cause-effect link is the same as prediction. If a point movement can be predicted, then the predictions can be used in planning to reach a goal.
        A prediction usually is made with respect to a set of points. A requirement: ais should be able to discriminate the set of points in the scene by the appearance.
        A prediction can be more (rigid body) or less (liquid, gas) precise.
        Definition: An object is a set of points, which can be discriminated by appearance from a scene and which properties (location, temperature, color) can be more or less predicted
                    and the prediction of a point is constrained by predictions of other points of the set.
        Definition: A refernce (landmark) object is a set of points, which can be discriminated by appearance from a scene, and thus can be used as a reference when describing movement of objects.
        E.g.: A movement of a rigid body point P places the constraint on all other body points Q_i based on the constraint |P - Q_i| = const_i
        To discover that movement of one point imposes constraints on movement of another point, these points need to be selected and their movement analyzed. In rigid body it means that 
        every body point has to compare with every other body point. This implies the ability to identify a point in time. But not all points are identifiable based on the color of their nbhd.
        
        A point can be identified by location and color. But:
        - ais observes may be observing only some of the points, or area samples of the points, due to finite resolution of its sensors.
        - location of points may change in time.
        - color of points may change in time.
        
        Axiom: each point is assigned a unique identity, which is preserved through the point liftime. The point may change its properties (location, color), but it preserves the identity.
        A set of points at time t is descibed as a collection {{identity, location, color}}(t).
        Suppose, locations and color of points of the set are transformed as: {{identity, location, color}}(t) to {{identity, location, color}}(dt). Then a point with the identity id
        changed from identity.location(t), identity.color(t) to identity.location(t+dt), identity.color(t+dt).
        
        Statement: In general transformation, when any point can move anywhere and change to any color, it is impossible to identify points from location and color information.
        Proof: Supposed identity of points are known at any time t < tau: identity(location,color, t)=id, where id is a unique value for given t, so that 
               identity(location_i,color_i, t) != identity(location_j,color_j,t) for any location(t),color(t).
               The possibility of identification from location and color without constraints on the "t to tau" transformation means that there is a calculation procedure, which takes 
               the input location(t),color(t), identity(location(t),color(t),t), t < tau and location(tau),color(tau) and its output 
               calc_identity(location(tau),color(tau), tau) equals identity(location(tau),color(tau), tau), which is true identity of the points. 
               Suppose such calculation procedure exists. Suppose two points at the time tau swapped their location and color, preserving properties. Then if before the swap 
               calc_identity(location(tau),color(tau), tau)=true_identity(location(tau),color(tau), tau), then after the swap 
               calc_identity(location(tau),color(tau), tau)!=true_identity(location(tau),color(tau), tau), so that calc_identity is wrong.   Q.E.D.
        
        However, if constraints are assumed on the transformation of points' properties between times t and tau, then the identity calculation procedure may exist for some points.
        Example 1: the point set consists of points on the circle and the circle center. The transformation is constrained to the isometry, i.e. distance between points do not change.
        Then the center of the circle can be identified at any time.
        Example 2: each point of the set has a unique color. The transformation is constrained to preserving the color of each point. 
                 Then there is 1-to-1 mapping between a point color and identity and the identity calculation procedure implements this map.
        Example 3: the point set consists of points on two non-concentric circles of different radiuses and their centers. The transformation is constrained to the isometry. 
                 Then the centers of the circles can be found as points at the same distance from multiple points and the other points can be narrowed to at most 2 by measuring distances 
                 from the previously identified centers.
        Example 4: some points of the set have a unique color. The transformation is constrained to preserving the color of each point and isometry.
                 Then the points of unique color can be identified using the constraint preserving color of each point.
                 The points with non-unique color can be identified using the isometry constraint as preserving distances to the earlier identified points of unique color.
               
        Questions:
        1. Given a constraint on the transformation of the properties of the points of a set, how to find a procedure, calculating identity of at least some of the points 
           at time tau, given all point properies for t <= tau and identities for t < tau?
        2. Same as question 1, but for multiple constraints, acting at the same time. Note that different constraints may possible the identification procedures for different 
           subsets of the point set. 
        3. Same as question 1, but the identification procedure is allowed to significantly narrow the identity of the points, not necessarily to one identity. See the example 3.
        4. The scene may contain multiple sets, for each of which a constraint and corresponding identification procedure exists, but different constraint and different 
           identification procedure for different sets. How to find these sets in the scene?
        5. Same as question 4, but scenes for different times may contain subsets of sets for which the transformation constraint and the identification procedure exist.
        6. Suppose the transformation constraints are selected and corresponding sets of points and identification procedures are found. How to predict the points for a future time?
        
        Statement: Suppose one of the transformation constraints of a set of points is the isometry. Then to predict the location of the set it's necessary and sufficient to predict 
           the location of some 4 points in general position (noncollinear, nonplanar?) of the set.
        Proof: If the locations of all set points are predicted, then the locations of any 4 points of the set are predicted.
               If the locations of some 4 noncollinear points of the set are predicted, then coordinates of other points can be restored from equations |x-p_i|=d_i.
               The visual proof is the following: x is at distance d_i from p_i. Then x is at the intersection of the spheres (p_1,d_1) and (p_2,d_2), which is a point or a curve C12.
               x is also at the intersection of the curve C12 and the sphere (p_3,d_3), which is a point or 2 points. Finally x is selected as one of 2 points by selecting the one which 
               distance to the p_4 is d_4.
        
        An isometry of a set is a combination of a rotation (orthogonal transformation) and translation. Both rotation and translation are defined by 3 parameters, 
        so the isometry is defined by 6 parameters.
        Isometry transformation is calcualted as X = R*(X-Xc)+Xc+T where C is the point, which is a center of rotation, Xc its coordinate in the world reference frame,  
        X is a coordinate of a point of the set in the world reference frame , R is rotation matrix, T is translation. Note that after the transformation Xc=Xc+T. 
        Note also that another point can be selected as a center of the next rotation.
        To predict the isometry its 6 parameters have to be predicted.
        
        The isometry assumes that the point set is defined. In reality a scene contains multiple rigid bodies, each having its own isometry. 
        Assume all scene points satsify the unique color preservation constraint.
        One way to discover the sets with independent isometries: 
        - using unique color constraint, for each point of the scene at time t, find corresponding point candidates in the scene at time t+delta. 
        - for a given point only those points are in the same isometry set as the point, which preserve distance to the point. 
         Select a point P in the scene t. Denote its corresponding point in the scene t+delta as P'. For each point Q of the scene t, measure the distances
         |Q-P| and |Q'-P'|. If the distances are the same, then such points Q are candidates on being in the same isometry set as P. To be in the same isometry set
        for sure the point Q need to preserve distance to 2(or 3?) more points of the set, which are not collinear with P.
        Each such candidate point Q forms 2-point isometry set with P. The isometry needs to be verified for the times tau > t+delta. The 2-point isometry may be 
        extendable to more points. A point may belong to 2 isometry sets. For example, a hinge(pivot) point is a common point of both the still base and the rotating part.
        
        Now assume all scene points satsify the color preservation (not necessarily unique) constraint. Also assume that all the scene points satisfy isometry constraint 
        within some subsets. Then the points of non-unique color are identified only up to belonging to the set of the same color, if using only the color preservation 
        constraint. The location isometry constraint can be used to identify such points by distance to the unique color points of the same isometry set.

        Assume, that the color and isometry constraints were used to identify all the scene points and find the isometry subsets. Suppose an isometry subset is a subset 
        of a robotic arm controlled with a rotational or linear joint. Such movements have one degree of freedom and require one real or integer variable for the control.
        Suppose the robot sends a signal to such joint, and identifies the isometry subset, undergoing a non-trivial transformation. A linear transformation is described
        by 3 parameters. A rotational transformation can be described by its fixed point (3 parameters), and angular velocity vector, which includes the direction of
        rotation and signed velocity magnitude (3 parameters). The occurring events are (see event_type):
        - control signal u (from R1) acting between times t1,t2;
        - isometry point set rotating around a fixed point P with angular velocity w between times t1+d,t2+d;
        ais should establish the causal link between the events and learn the map between parameters of the events.
       
        Assume, that the color and isometry constraints were used to identify all the scene points and find the isometry subsets. Suppose ais discovered two isometry subsets,
        and their transformations. One isometry point set underwent transformation described by selected point (3 parameters), rotating around the point (3 parameters) and
        translating (3 parameters) within the time interval [t1,t2]. The second isometry set's transformation is described similarly. Suppose the sets came into contact.
        ais should establish the causal link between the events and learn the map between the parameters of the events.
        ais observations will depend on the phase (before the contact, contact, after the contact), intelligence of sets (none,1,or both sets are intelligent),material
        (elasticity, friction coefficient), shape, location of the contact area, magnitude and direction of the contact forces, external forces (gravity, air resistance)
        Suppose, that the observed sets are electrically neutral and small enough to ignore the gravitional forces between them. Then the events of the set movements
        are interdependent only when the sets contact each other. An isolated rigid body is described by Force-torque equations[1]:
        F=m*a    T=[IR]*alpha+omega x [IR]*omega, where a is the acceleration of a reference point, omega is the angular velocity, alpha is the angular acceleration,
        IR is the inertia matrix. From these equations, the parameters defining the movement of the rigid body are angular acceleration alpha (or angular velocity omega),
        linear acceleration a, starting condition (at t=0) for the location and velocity.
        

        
        
        

        Intuition:
         The outcome of the sets interaction should depend on the contact region between sets.The contact region becomes a subset of points, which are shared between the two 
         isometry sets. Suppose one set is a driver, that is its move drives the contact points of the other set. Using the rigid body mechanics, 
         Humans can calculate the expected movement of the second set writing the force-torque equations [1], calculating transla-
         tional and rotational movements. ais may have a number of levels at which the movements of the isometry sets (rigid bodies) is calculated:
         - automatic, fast but less precise estimation possibly found (learned) within a class of functions from observing interactions of various bodies;
         - elaborated, using analysis, which eventually comes to the rigid body mechanics;
         - semiautomatic, which is the automatic method augmented with heuristics learned from analysis of the observations and made automatic;
        ais would observe various results of collision depending on the velocities immediately before the contact between set. E.g., the grasping arm slows down to the 
        speed of the grasped object, and then proceeds to push or carry the object. 

        1. Rigid body dynamics:https://en.wikipedia.org/wiki/Rigid_body_dynamics      
        2. Preservation of linear and angular momentums:(https://en.wikipedia.org/wiki/Angular_momentum,https://en.wikipedia.org/wiki/Momentum).
        
    */           

    bool terminal_node = false;
    
    certainty_counter = 0;
    
    
    //todo: add preconditions on cause and effect, necessary for the plan execution to start, f.e. cause and event objects have to be in contact for the move of one to cause the move of the other
    
    plan_type(event_type cause, event_type effect)
    {
        this->cause = cause;
        this->effect = effect;
        terminal_node = true;
    }
    
    add_step_to_plan(plan_type plan_step)
    {
        plan_steps.push_back(plan_step);
    }
    
    execute_and_monitor()
    {
        if (terminal_node)
        {
            cause.execute_and_monitor();
            if (effect.occurred())
                return;
            
            // todo: modify plan and possible plans to account for the unexpected plan outcome
            
            return;
        }
        for (plan_step in plan_steps)
        {
            plan_step.execute_and_monitor();
            
            if (effect.occurred())
                continue;
            
            // todo: modify plan and possible plans to account for the unexpected plan outcome
        }
    }
}

class observed_scene_type
{
    list <object_point_type> points;
    list <control_signal_type> control_signals;    
    
    list<relation_type> relations;
    time_type time;
    
    capture()
    {
        
    }
    
    calculate_relations()
    {
    }
    
    relation_type find_matching_relation(relation_type relation)
    {
    }
    
    bool contains_point(object_point_type point)
    {
    }
    
    event_type calculate_object_from_relation_move(observed_scene_type observed_scene, relation_type relation)
    {
        relation_type matching_relation = find_matching_relation(relation);
        object_move_type object_move = matching_relation.calculate_object_move(relation);
        object_type object;
        object.add_relation(relation);
        for (auto scene_point in points) 
        {
            if (observed_scene.contains_point(scene_point.move(object_move))
                object.add_point(scene_point);
        }
        
        event_type event(object, object_move, observed_scene.time);
        return event;
    }
}

class ai_system
{
    list <goal_type> goals;
    list <plan_type> possible_plans;
    list <plan_type> executed_plans;
    list <observed_scene_type> observed_scenes;    
    list <object_type> objects;
    list <event_type> events;
    
    lifecycle()
    {
        observe_world();
        learn_possible_plans();
        calculate_goals();
        assemble_plans_to_achieve_goals();
        execute_and_monitor_plans();
        /*  todo:
            cause_effect_sequences (aka plan) are executed no matter if ais wants it or not unless ais is physically participating in it by sending signal to its body or messaging a command to another ais.
            ais monitors if the world changes according to its predictions (through cause_effect_sequences) and modifies them as necessary
            ais can form complex signals into symbols or messages to communicate with other aises and interpret the symbols and messages received from other aises.
            sensors: visual (location, color; objects, moving in space, changing color), 
                        characters, words, sentences, symbols
                     tactile (location, temperature; objects moving in space, changing temperature), 
                     auditory (location, sound pitch and level; objects moving in space, generate sound of various pitch and level)
                        characters, words, sentences are coded relations; 
                        the relations can encode events, including objects and object moves
            control signals:
                muscle signal to move a body part;
                characters, words, sentences to send visual, tactile, auditory messages
            tracking the changes of the objects of interest (maybe adding goals to track the objects?)
            analyzing the objects of inteterest (curiosity, maybe by adding "curiosity" goals?)
            looking for goals ("what shall i do next?")
            monitoring one's thoughts in humans is maybe an ability to monitor and participate in forming high-level plans to reach one's goals:
            - subconscious plan forming: low-level plan forming, like which muscle tissue to contract;
            - conscious plan forming: high-level plan forming, like which store to go;
            - automatic plan forming (and execution): which leg steps next, how to lock the door.
         */
    }

    observe_world()
    {
        observed_scene_type observed_scene;
        observed_scene.capture();
        observed_scene.calculate_relations();
        observed_scenes.push_back();
    }    
    
    learn_possible_plans() 
    {
        observed_scene_type observed_scene = observed_scenes.back();
        get_events_from_matching_observed_scene_relation_to_existing_objects();
        get_events_from_matching_observed_scene_relation_to_previous_scenes();  
        
        get_control_signal_events();
        
        get_new_and_correct_existing_possible_plans_from_events();
    }
    
    get_new_and_correct_existing_possible_plans_from_events()
    {
        for (event_type cause in events)
        {
            for (event_type effect in events)
            {
                if (cause.time < effect.time)
                {
                    if (possible_plans[cause].effect != effect)
                    {
                        possible_plans[cause].certainty_counter--;
                        
                        plan_type possible_plan(cause, effect);
                        possible_plans.push_back(possible_plan);
                    } 
                    else
                    {
                        possible_plans[cause].certainty_counter++;                    
                    }
                  
                }
            }
        }
    }

    get_control_signal_events()
    {
        observed_scene_type observed_scene = observed_scenes.back();
        
        for (control_signal in observed_scene.control_signals)
        {
            event_type event(observed_scene.control_signal, observed_scene.time);
            events.push_back(event);                
        }
    }
    
    get_events_from_matching_observed_scene_relation_to_existing_objects()
    {
        observed_scene_type observed_scene = observed_scenes.back();
        for (relation in observed_scene.relations)
        {
            for (object in objects)
            {
              event_type event = object.calculate_event_from_relation_move(observed_scene, relation);
              events.push_back(event);                
            }
        }    
    }
    
    get_events_from_matching_observed_scene_relation_to_previous_scenes()
    {
        auto it = observed_scenes.rbegin();
        observed_scene_type observed_scene = *it++;
        observed_scene_type prev_observed_scene = *it;
        
        for (relation in observed_scene.relations)
        {
            event_type event = prev_observed_scene.calculate_object_from_relation_move(observed_scene, relation);
            events.push_back(event);
            objects.push_back(object);
            // todo: environment as an object, an object is a part of another object, event, causality between events
        }
    }
    
    assemble_plans_to_achieve_goals()
    {
        for (goal in goals) 
        {
            plan_type plan;
            subgoal = goal;
            while (!achieved(subgoal))
            {
                plan_step = get_plan_by_effect(subgoal);
                plan.add_step_to_plan(plan_step);
                subgoal = plan_step.cause;
            }
            plans.push_back(plan);
        }
    }

    execute_and_monitor_plans();
    {
        for (plan in executed_plans) 
        {
            plan.execute_and_monitor();
        }
    }

    plan_type get_plan_by_effect(event_type effect)
    {
        for (plan in possible_plans) 
        {
            if (plan.effect == effect)
                return plan_step;
        }
        return null;
    }
    
    predict_world();
    calculate_goals();
}


Detection algorithm:


input sensor data frame (scene);
match_observed_scene_relation_to_existing_objects
match_observed_scene_relation_to_previous_scenes


match_observed_scene_relation_to_existing_objects

for each obj having a detected rel
    for each subset of rels which satisfy the obj rel poses
        find transform between obj and scene
        if the scene data and obj after transform are color compatible
            the object is identified
        else if the scene data and obj after transform are partially color compatible
        
        exceptions:
            break;
        
        
        
