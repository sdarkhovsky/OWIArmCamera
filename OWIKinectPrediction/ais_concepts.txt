predict_objs (time t)
    for each object
        object.state(t) = object.predict_model(t)
    

class object_point_type
{
    location_type location;
    color_type color;
}

class object_move_type
{
    /*
        location change,
        send control signal to its joints
        communicating message
    */
    
    object_start_state;
    object_end_state;
        
    vector3 acceleration;
}

class relation_type
{
    location_type location;
    vector3 orientation;
    object_move_type calculate_object_move(relation_type)
    {
    }
}

class object_type
{
    list<object_point_type> points;
    list<relation_type> relations;
    
    // todo: object may contain other objects (subobjects), which move together with the parent object.
    
    predict_model(time t)
    {
       // todo: this prediction model is going to be used somewhere, but where?
        acceleration(t) = force/mass;
        velocity(t) = object.velocity(t_prev)+object.acceleration(t)*(t-t_prev);
        location(t) = location(t_prev) + velocity(t)*(t-t_prev);
    }
    
    add_relation(relation_type relation )
    {
        relations.push_back(relation);
    }
    
    add_point(object_point_type point)
    {
        points.add_point(point);
    }
    
    bool valid()
    {
        return (points.size()>threshold);
    }

    relation_type find_matching_relation(relation_type relation)
    {
    }
    
    update_state(object_move_type object_move)
    {
    }
    
    event_type calculate_event_from_relation_move(observed_scene_type observed_scene, relation_type relation)
    {
        relation_type matching_relation = find_matching_relation(relation);
        object_move_type object_move = matching_relation.calculate_object_move(relation);

        for (auto object_point in points) 
        {
            if (!observed_scene.contains_point(object_point.move(object_move))
                return null;
        }

        object.update_state(object_move);
        
        event_type event(object, object_move, observed_scene.time);
        return event;
    }    
}

class control_signal_type
{
    joint_type joint;
    signal_value_type value;
}

class event_type
{
    control_signal_type control_signal;
    object_type object;
    object_move_type move;
    time_type time;
    
    bool control_signal_event = false;
    
    event_type(object_type object, object_move_type move, time_type time)
    {
        this->object = object;
        this->move = move;
        this->time = time;
    }
    
    event_type(control_signal_type control_signal, time_type time)
    {
        this->control_signal = control_signal;
        this->time = time;
        control_signal_event = true;
    }
    
    bool occurred()
    {
        if (object.state == move.object_end_state)
            return true;
            
        return false;
    }
    
    execute_and_monitor()
    {
        if (control_signal_event)
        {
            send(control_signal.joint, control_signal.value);
            return;
        }
    }
}

class plan_type
{
    list<plan_type> plan_steps;
    
    event_type cause;
    event_type effect;
    //todo: need to add precondition? plan/prediction_model: object precondition and cause, object_move, object's end state (effect)
    // todo: messaging (voice, gesture, ...) is a type of a cause of an intelligent object move
    /* todo: an expected effect may not always follow a cause. In such cases, an additional hidden cause can be presumed. Such hidden cause may be an effect of another observable or hidden cause.
             A plan to achieve an effect may be a graph with multiple causes and side effects:
             cause - node - node - node - node - node - node - effect
                    cause ---|                           |--- side_effect
                             |-- side_effect
       Some nodes (causes and effects) of the graph can be hidden. In such cases, they need to be estimated. For example, a human may be corrupted, evil, hungry, etc.
       The hidden events (or states) in intelligent systems may be made explicit using verbal messages.
       
        events:
        visual observations measure points locations and color. Corresponding events are changes in point locations and color: 
            (point_location(curr_time)=>point_location(future_time)), (point_color(curr_time)=>point_color(future_time))
            This assumes that a point is identified and its location is traced in time.
            The observation describes a point by its properties (location, color)(time) and by the properties (location, color)(time) of surrounding points.
            A decision needs to be made: given the  location and color of a point and its neighbourhood at tau < t and unidentified point and its neighbourhood at time t, 
            find a point in the past (tau < t), which has the same identity.
            It's possible that some set of points at curr_time can be mapped to a set of points in the future_time even though not all individual points of the set can be precisely mapped:
            some points are mapped exactly, some approximately.
            Decision-making is a process resulting in the selection of a belief or a course of action among several alternative possibilities (https://en.wikipedia.org/wiki/Decision-making)
            Selection of the map's target point is made among the points of the scene observed at the future_time.
            Finding the mapping function can be looked upon as an optimization problem: find f(x):X->Y, such that f = min (by g) J(g), here f is a function, which minimizes the criteria J by g, 
            where g is from some class of functions. The calculation performed by the optimal function f could be multi-step. For example, given the f's domain D and range R, and a point x from D, 
            step 1: find points in R, which are close by location to x; step 2: find a point in R among those selected at step 1, which is closest by color to the point x. Instead of finding 
            an optimal choice, the decision can be to find just a good choice. 
            
            The identification of a point (in the current scene with a point in the previous scene) and finding causes of its location and color change and how they change may be solved 
            simultaneously. Or it could be solved in a loop:
            
            identify a point in the current scene, which matches a given point in the previous scene;
            find the prediction law of the point in the future scenes: prediction(cause)=effect;
            loop
            {
                predict the scene;
                observe the prediction;
                if error in prediction
                {
                    identify a point in the current scene, which matches a given point in the previous scene;
                    find the prediction law of the point in the future scenes: prediction(cause)=effect;
                }
            }
            If the identification and prediction laws in the loop are selected by uniform random sampling, then the chances of finding the right ones are very slim.
            The recurrent identification and prediction should be a converging process, not necessarily stricktly monotonically.
            
            A cost can be associated with an option. When the option's cost is lowest or/and below a threshold, the option is selected.
            Selection of a point at future_time, which is close by color and location from a point at curr_time is motivated by a belief (hypothesis) 
            that a material point is not likely to change its properties very fast. This hypothesis is confirmed later by ability to predict a point behaviour in most of the cases.
            
            How this hypothesis is represented in ais? How is it applied?
            
            
            This shows that ais has to have a set of hypotheses even before it starts learning them from observations. (111111111111111111111)
            
            
            Intuitively, cont here 111111111111111111111111111: continuous location and color change, preserves some statistic(s) evaluated on a nbhd,...
            
            
            
            
        tactile observations measure points location and temperature. Corresponding events are changes in point locations and temperature.
        auditory observations measure points location, sound volume and frequency. Corresponding events are changes in point locations, volume and frequency. 
        
        
    */           

    bool terminal_node = false;
    
    certainty_counter = 0;
    
    
    //todo: add preconditions on cause and effect, necessary for the plan execution to start, f.e. cause and event objects have to be in contact for the move of one to cause the move of the other
    
    plan_type(event_type cause, event_type effect)
    {
        this->cause = cause;
        this->effect = effect;
        terminal_node = true;
    }
    
    add_step_to_plan(plan_type plan_step)
    {
        plan_steps.push_back(plan_step);
    }
    
    execute_and_monitor()
    {
        if (terminal_node)
        {
            cause.execute_and_monitor();
            if (effect.occurred())
                return;
            
            // todo: modify plan and possible plans to account for the unexpected plan outcome
            
            return;
        }
        for (plan_step in plan_steps)
        {
            plan_step.execute_and_monitor();
            
            if (effect.occurred())
                continue;
            
            // todo: modify plan and possible plans to account for the unexpected plan outcome
        }
    }
}

class observed_scene_type
{
    list <object_point_type> points;
    list <control_signal_type> control_signals;    
    
    list<relation_type> relations;
    time_type time;
    
    capture()
    {
        
    }
    
    calculate_relations()
    {
    }
    
    relation_type find_matching_relation(relation_type relation)
    {
    }
    
    bool contains_point(object_point_type point)
    {
    }
    
    event_type calculate_object_from_relation_move(observed_scene_type observed_scene, relation_type relation)
    {
        relation_type matching_relation = find_matching_relation(relation);
        object_move_type object_move = matching_relation.calculate_object_move(relation);
        object_type object;
        object.add_relation(relation);
        for (auto scene_point in points) 
        {
            if (observed_scene.contains_point(scene_point.move(object_move))
                object.add_point(scene_point);
        }
        
        event_type event(object, object_move, observed_scene.time);
        return event;
    }
}

class ai_system
{
    list <goal_type> goals;
    list <plan_type> possible_plans;
    list <plan_type> executed_plans;
    list <observed_scene_type> observed_scenes;    
    list <object_type> objects;
    list <event_type> events;
    
    lifecycle()
    {
        observe_world();
        learn_possible_plans();
        calculate_goals();
        assemble_plans_to_achieve_goals();
        execute_and_monitor_plans();
        /*  todo:
            cause_effect_sequences (aka plan) are executed no matter if ais wants it or not unless ais is physically participating in it by sending signal to its body or messaging a command to another ais.
            ais monitors if the world changes according to its predictions (through cause_effect_sequences) and modifies them as necessary
            ais can form complex signals into symbols or messages to communicate with other aises and interpret the symbols and messages received from other aises.
            sensors: visual (location, color; objects, moving in space, changing color), 
                        characters, words, sentences, symbols
                     tactile (location, temperature; objects moving in space, changing temperature), 
                     auditory (location, sound pitch and level; objects moving in space, generate sound of various pitch and level)
                        characters, words, sentences are coded relations; 
                        the relations can encode events, including objects and object moves
            control signals:
                muscle signal to move a body part;
                characters, words, sentences to send visual, tactile, auditory messages
            tracking the changes of the objects of interest (maybe adding goals to track the objects?)
            analyzing the objects of inteterest (curiosity, maybe by adding "curiosity" goals?)
            looking for goals ("what shall i do next?")
            monitoring one's thoughts in humans is maybe an ability to monitor and participate in forming high-level plans to reach one's goals:
            - subconscious plan forming: low-level plan forming, like which muscle tissue to contract;
            - conscious plan forming: high-level plan forming, like which store to go;
            - automatic plan forming (and execution): which leg steps next, how to lock the door.
         */
    }

    observe_world()
    {
        observed_scene_type observed_scene;
        observed_scene.capture();
        observed_scene.calculate_relations();
        observed_scenes.push_back();
    }    
    
    learn_possible_plans() 
    {
        observed_scene_type observed_scene = observed_scenes.back();
        get_events_from_matching_observed_scene_relation_to_existing_objects();
        get_events_from_matching_observed_scene_relation_to_previous_scenes();  
        
        get_control_signal_events();
        
        get_new_and_correct_existing_possible_plans_from_events();
    }
    
    get_new_and_correct_existing_possible_plans_from_events()
    {
        for (event_type cause in events)
        {
            for (event_type effect in events)
            {
                if (cause.time < effect.time)
                {
                    if (possible_plans[cause].effect != effect)
                    {
                        possible_plans[cause].certainty_counter--;
                        
                        plan_type possible_plan(cause, effect);
                        possible_plans.push_back(possible_plan);
                    } 
                    else
                    {
                        possible_plans[cause].certainty_counter++;                    
                    }
                  
                }
            }
        }
    }

    get_control_signal_events()
    {
        observed_scene_type observed_scene = observed_scenes.back();
        
        for (control_signal in observed_scene.control_signals)
        {
            event_type event(observed_scene.control_signal, observed_scene.time);
            events.push_back(event);                
        }
    }
    
    get_events_from_matching_observed_scene_relation_to_existing_objects()
    {
        observed_scene_type observed_scene = observed_scenes.back();
        for (relation in observed_scene.relations)
        {
            for (object in objects)
            {
              event_type event = object.calculate_event_from_relation_move(observed_scene, relation);
              events.push_back(event);                
            }
        }    
    }
    
    get_events_from_matching_observed_scene_relation_to_previous_scenes()
    {
        auto it = observed_scenes.rbegin();
        observed_scene_type observed_scene = *it++;
        observed_scene_type prev_observed_scene = *it;
        
        for (relation in observed_scene.relations)
        {
            event_type event = prev_observed_scene.calculate_object_from_relation_move(observed_scene, relation);
            events.push_back(event);
            objects.push_back(object);
            // todo: environment as an object, an object is a part of another object, event, causality between events
        }
    }
    
    assemble_plans_to_achieve_goals()
    {
        for (goal in goals) 
        {
            plan_type plan;
            subgoal = goal;
            while (!achieved(subgoal))
            {
                plan_step = get_plan_by_effect(subgoal);
                plan.add_step_to_plan(plan_step);
                subgoal = plan_step.cause;
            }
            plans.push_back(plan);
        }
    }

    execute_and_monitor_plans();
    {
        for (plan in executed_plans) 
        {
            plan.execute_and_monitor();
        }
    }

    plan_type get_plan_by_effect(event_type effect)
    {
        for (plan in possible_plans) 
        {
            if (plan.effect == effect)
                return plan_step;
        }
        return null;
    }
    
    predict_world();
    calculate_goals();
}


Detection algorithm:


input sensor data frame (scene);
match_observed_scene_relation_to_existing_objects
match_observed_scene_relation_to_previous_scenes


match_observed_scene_relation_to_existing_objects

for each obj having a detected rel
    for each subset of rels which satisfy the obj rel poses
        find transform between obj and scene
        if the scene data and obj after transform are color compatible
            the object is identified
        else if the scene data and obj after transform are partially color compatible
        
        exceptions:
            break;
        
        
        
